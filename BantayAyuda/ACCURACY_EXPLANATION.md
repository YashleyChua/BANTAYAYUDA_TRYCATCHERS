# Model Accuracy Explanation

## The Critical Question: How Do We Know If Predictions Are Accurate?

### Current Situation

**The model shows high training accuracy (F1 = 1.0, MultiClass error ≈ 0.0005), BUT:**

1. **Trained on Synthetic Data**
   - The model was trained on 10,000 synthetic samples
   - "Ground truth" labels were generated by rules, not real damage assessments
   - Example rule: `if flood_depth > 3.5m: damage = TOTAL`

2. **No Real Validation Data**
   - We don't have actual field damage assessments to validate against
   - We can't measure "real-world accuracy" without ground truth labels

3. **What We Can Measure**
   - **Training Accuracy**: How well model fits synthetic data (100% F1 score)
   - **Rule-Based Agreement**: How well ML matches rule-based system
   - **NOT Real Damage Accuracy**: We can't measure this without real assessments

---

## What the Model Actually Predicts

The model predicts **ECT amounts** (₱0, ₱5K, ₱10K), not directly "damage or not".

**Input Features:**
- Flood depth (meters)
- House height/width
- Barangay location
- 4Ps recipient status
- Damage classification (if available)

**Output:**
- ECT allocation amount (0, 5000, or 10000)

---

## How to Measure Real Accuracy

### Option 1: Field Validation (Best)
1. Deploy model on real disaster
2. Get actual damage assessments from DSWD/MSWDO
3. Compare ML predictions vs. real assessments
4. Calculate accuracy metrics

### Option 2: Historical Data Validation
1. Use past disaster data with known outcomes
2. Run model on historical cases
3. Compare predictions vs. actual ECT allocations
4. Measure accuracy on real data

### Option 3: Expert Review
1. Have DSWD assessors review ML predictions
2. Get expert feedback on accuracy
3. Adjust model based on expert input

---

## Current Validation Approach

The `validate_model.py` script compares:
- **ML Predictions** vs **Rule-Based Assessments**

This tells us:
- ✅ How consistent ML is with rule-based system
- ❌ NOT how accurate ML is on real damage

**Result**: 100% agreement with rule-based system (because both use similar logic)

---

## Limitations & Recommendations

### ⚠️ Current Limitations:
1. No real ground truth damage data
2. Model trained on synthetic data
3. Can't measure real-world accuracy
4. Feature mismatch issues (categorical features)

### ✅ Recommendations:
1. **Collect Real Data**: Partner with DSWD to get historical disaster data
2. **Field Testing**: Deploy on small pilot disaster to validate
3. **Expert Review**: Have assessors review ML predictions
4. **Continuous Learning**: Update model with real assessment feedback

---

## Training Metrics (Synthetic Data)

From `ect_allocation_model-main/notebooks/catboost_info/`:

- **TotalF1 Score**: 1.0 (100% on test set)
- **MultiClass Error**: ~0.0005 (very low)
- **Training Iterations**: 1000

**Note**: These metrics are on synthetic test data, not real damage assessments.

---

## Bottom Line

**We DON'T know real accuracy yet** because:
- Model trained on synthetic data
- No real damage assessments to validate against
- Can only measure agreement with rule-based system

**To know real accuracy, we need:**
1. Real damage assessment data
2. Field validation
3. Historical disaster data with outcomes

The model is **ready for testing** but needs **real-world validation** to measure true accuracy.

